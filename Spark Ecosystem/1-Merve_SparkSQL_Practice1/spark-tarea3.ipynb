{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark - Tarea 3\n",
    "### Practice 1: Task 3: We will continue working with the Heterogeneity Dataset for Human Activity Recognition (HHAR) (full dataset), which contains motion sensor data from smartphones and smartwatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import*\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Merve_Homework\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question1\n",
    "Generate a Parquet file for each of the original CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/02 23:47:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/12/02 23:47:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/12/02 23:47:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/12/02 23:47:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/12/02 23:47:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "23/12/02 23:47:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/12/02 23:47:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/12/02 23:47:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/12/02 23:47:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/12/02 23:47:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/12/02 23:47:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/12/02 23:47:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/12/02 23:47:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/12/02 23:47:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "23/12/02 23:47:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/12/02 23:47:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/12/02 23:47:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/12/02 23:47:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/12/02 23:47:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/12/02 23:47:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/12/02 23:47:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/12/02 23:47:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/12/02 23:47:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "23/12/02 23:47:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/12/02 23:47:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/12/02 23:47:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/12/02 23:47:20 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/12/02 23:47:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "23/12/02 23:47:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/12/02 23:47:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/12/02 23:47:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/12/02 23:47:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 63.33% for 12 writers\n",
      "23/12/02 23:47:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 69.09% for 11 writers\n",
      "23/12/02 23:47:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "23/12/02 23:47:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "23/12/02 23:47:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a function to convert the csv file to a parquet file\n",
    "def write_csv_into_parquet(csv_file):\n",
    "    \"\"\"\n",
    "    This function writes data of a csv file into a parquet file.\n",
    "    param csv_file: csv file\n",
    "    return: None\n",
    "    \"\"\"\n",
    "    parquet_file = csv_file.replace(\".csv\", \".parquet\")\n",
    "    df = spark.read.csv(csv_file, header=False, inferSchema=True)\n",
    "    df.write.parquet(parquet_file)\n",
    "\n",
    "\n",
    "write_csv_into_parquet(\"activity_recognition_exp/Phones_accelerometer.csv\")\n",
    "write_csv_into_parquet(\"activity_recognition_exp/Phones_gyroscope.csv\")\n",
    "write_csv_into_parquet(\"activity_recognition_exp/Watch_accelerometer.csv\")\n",
    "write_csv_into_parquet(\"activity_recognition_exp/Watch_gyroscope.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a comparison table showing the sizes of each CSV file and its corresponding Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| File                           | Csv (byte) | Parquet (byte)  | Ratio (Csv/Pq)       |\n",
      "────────────────────────────────────────────────────────────────────────────────────────\n",
      "| Phones_accelerometer           | 1291856327 | 4096            | 315394.6110839844    |\n",
      "| Phones_gyroscope               | 1379145657 | 4096            | 336705.4826660156    |\n",
      "| Watch_accelerometer            | 327168052  | 4096            | 79875.0126953125     |\n",
      "| Watch_gyroscope                | 308337025  | 4096            | 75277.59399414062    |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file1_csv = 'activity_recognition_exp/Phones_accelerometer.csv'\n",
    "file2_csv = 'activity_recognition_exp/Phones_gyroscope.csv'\n",
    "file3_csv = 'activity_recognition_exp/Watch_accelerometer.csv'\n",
    "file4_csv = 'activity_recognition_exp/Watch_gyroscope.csv'\n",
    "\n",
    "file1_pq = 'activity_recognition_exp/Phones_accelerometer.parquet'\n",
    "file2_pq = 'activity_recognition_exp/Phones_gyroscope.parquet'\n",
    "file3_pq = 'activity_recognition_exp/Watch_accelerometer.parquet'\n",
    "file4_pq = 'activity_recognition_exp/Watch_gyroscope.parquet'\n",
    "\n",
    "csv_files = [file1_csv, file2_csv, file3_csv, file4_csv]\n",
    "pq_files = [file1_pq, file2_pq, file3_pq, file4_pq]\n",
    "\n",
    "# Get file size and compare them in Table Format\n",
    "print(\"\")\n",
    "print('| {:<30} | {:<10} | {:<15} | {:<20} |'.format(*[\"File\", \"Csv (byte)\", \"Parquet (byte)\", \"Ratio (Csv/Pq)\"]))\n",
    "print(u'\\u2500' * 88)\n",
    "for file_csv, file_pq in zip(csv_files, pq_files):\n",
    "    size_csv = os.path.getsize(file_csv)\n",
    "    size_pq = os.path.getsize(file_pq)\n",
    "    ratio = size_csv/size_pq\n",
    "\n",
    "    print('| {:<30} | {:<10} | {:<15} | {:<20} |'.format(file_csv.replace(\".csv\", \"\").split(\"/\")[1], size_csv, size_pq, ratio))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question2\n",
    "Measure Execution Time for Different Data Loading Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| File                           | Csv (sec)       | Parquet (sec)   | Ratio (Csv/Pq)       |\n",
      "─────────────────────────────────────────────────────────────────────────────────────────────\n",
      "| Phones_accelerometer           | 3.310691        | 0.045383        | 72.95002533988497    |\n",
      "| Phones_gyroscope               | 3.446976        | 0.032276        | 106.79687693642335   |\n",
      "| Watch_accelerometer            | 0.87495         | 0.031636        | 27.656783411303582   |\n",
      "| Watch_gyroscope                | 0.793566        | 0.031213        | 25.42421426969532    |\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Read parquet files and strore compile time for each file\n",
    "compile_time_pq = []\n",
    "for file_pq in pq_files:\n",
    "    start_time = datetime.datetime.now()\n",
    "    df = spark.read.parquet(file_pq)\n",
    "    end_time = datetime.datetime.now()\n",
    "    compile_time_pq.append((end_time - start_time).total_seconds())\n",
    "\n",
    "\n",
    "# Read csv files and strore compile time for each file\n",
    "compile_time_csv = []\n",
    "for file_csv in csv_files:\n",
    "    start_time = datetime.datetime.now()\n",
    "    df = spark.read.csv(file_csv, header=False, inferSchema=True)\n",
    "    end_time = datetime.datetime.now()\n",
    "    compile_time_csv.append((end_time - start_time).total_seconds())\n",
    "\n",
    "# Print compile time for each file in Table Format\n",
    "print(\"\")\n",
    "print('| {:<30} | {:<15} | {:<15} | {:<20} |'.format(*[\"File\", \"Csv (sec)\", \"Parquet (sec)\", \"Ratio (Csv/Pq)\"]))\n",
    "print(u'\\u2500' * 93)\n",
    "for file_csv, file_pq, time_csv, time_pq in zip(csv_files, pq_files, compile_time_csv, compile_time_pq):\n",
    "    ratio = time_csv/time_pq\n",
    "    print('| {:<30} | {:<15} | {:<15} | {:<20} |'.format(file_csv.replace(\".csv\", \"\").split(\"/\")[1], time_csv, time_pq, ratio))\n",
    "print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
